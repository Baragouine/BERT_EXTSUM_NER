{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bert for extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from time import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import statistics\n",
    "import os\n",
    "from utils.split_all_docs import split_all_docs\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.DataLoader import DataLoader\n",
    "from utils.preprocess_df import preprocess_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook() -> bool:\n",
    "  try:\n",
    "    shell = get_ipython().__class__.__name__\n",
    "    if shell == 'ZMQInteractiveShell':\n",
    "      return True   # Jupyter notebook or qtconsole\n",
    "    elif shell == 'TerminalInteractiveShell':\n",
    "      return False  # Terminal running IPython\n",
    "    else:\n",
    "      return False  # Other type (?)\n",
    "  except NameError:\n",
    "    return False      # Probably standard Python interpreter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Hyper-)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse args if script mode\n",
    "parser = argparse.ArgumentParser(description='extractive summary and ner using bert')\n",
    "\n",
    "parser.add_argument('-is_graphic',type=int,default=1,choices=[0,1])\n",
    "parser.add_argument('-gpu_num',type=int,default=0)\n",
    "parser.add_argument('-batch_size',type=int,default=4)#32)\n",
    "parser.add_argument('-epochs',type=int,default=100)\n",
    "\n",
    "args = None\n",
    "\n",
    "if is_notebook():\n",
    "  args = parser.parse_args(\"\")\n",
    "else:\n",
    "  args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse:\n",
      "is_graphic: True\n",
      "cuda_num: 0\n",
      "epochs 100\n",
      "batch_size 4\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "is_graphic = args.is_graphic != 0\n",
    "cuda_num = args.gpu_num\n",
    "bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# hyper-parameters\n",
    "batch_size = args.batch_size\n",
    "epochs = args.epochs\n",
    "learning_rate = 1e-3\n",
    "early_stopping = 3\n",
    "model_name = \"06-train_bertbase_ext_summary_DailyMail\"\n",
    "sub_folder_name = \"model_name__{}__time__{}__lr__{}__batch_size__{}__cuda_num__{}__early_stopping__{}\".format(model_name, time(), learning_rate, batch_size, cuda_num, early_stopping)\n",
    "checkpoints_folder = \"./checkpoints/\" + sub_folder_name\n",
    "loss_sum_coef = 0.5\n",
    "loss_ner_coef = 0.5\n",
    "average_number_of_sentences_per_document = 3\n",
    "\n",
    "# print\n",
    "print(\"parse:\")\n",
    "print(\"is_graphic:\", is_graphic)\n",
    "print(\"cuda_num:\", cuda_num)\n",
    "print(\"epochs\", epochs)\n",
    "print(\"batch_size\", batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 6\n",
      "GPU 0: NVIDIA GeForce GTX 1080 Ti\n",
      "GPU 1: NVIDIA GeForce GTX 1080 Ti\n",
      "GPU 2: NVIDIA GeForce GTX 1080 Ti\n",
      "GPU 3: NVIDIA GeForce GTX 1080\n",
      "GPU 4: NVIDIA GeForce GTX 1080\n",
      "GPU 5: NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "  # Display the number of available GPUs\n",
    "  print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "  # Display the name of each GPU\n",
    "  for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "  print(\"MPS available.\")\n",
    "else:\n",
    "  print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:\" + str(cuda_num) \n",
    "elif torch.backends.mps.is_available():\n",
    "  dev = torch.device(\"mps\")\n",
    "else:  \n",
    "  dev = \"cpu\" \n",
    "\n",
    "device = torch.device(dev)\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(l):\n",
    "  return sum(l) / len(l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json(\"./data/test.json\")\n",
    "df_val = pd.read_json(\"./data/val.json\")\n",
    "df_train = pd.read_json(\"./data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats corpus\n",
    "\n",
    "if False:\n",
    "  len_articles = []\n",
    "\n",
    "  for idx in df.index:\n",
    "    txt = df[\"flat_contents\"][idx]\n",
    "    txt = sent_tokenize(txt)\n",
    "    txt = \" [SEP] \".join(txt)\n",
    "    txt = bert_tokenizer.encode(txt, add_special_tokens=False)\n",
    "    len_articles.append(len(txt))\n",
    "\n",
    "  print(\"max:\", max(len_articles), \", mediane:\", statistics.median(len_articles), \", avg:\", average(len_articles), \", std:\", statistics.stdev(len_articles))\n",
    "  # max: 184812 , mediane: 862.5 , avg: 2146.653203237274 , std: 4145.829631357804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summaries</th>\n",
       "      <th>labels</th>\n",
       "      <th>own_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>by daily mail reporter last updated at 11:49 a...</td>\n",
       "      <td>costs of ' air force 2 ' flight amount to over...</td>\n",
       "      <td>[1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>with the trademark curly hair and giant person...</td>\n",
       "      <td>jones won his 100th cap for wales during the s...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  by daily mail reporter last updated at 11:49 a...   \n",
       "1  with the trademark curly hair and giant person...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  costs of ' air force 2 ' flight amount to over...   \n",
       "1  jones won his 100th cap for wales during the s...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, ...   \n",
       "1  [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, ...   \n",
       "\n",
       "                                          own_labels  \n",
       "0  [1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_df(df=df_train, bert_tokenizer=bert_tokenizer, block_size=512, trunc_doc=50, doc_column_name=\"text\", labels_sum_column_name=\"own_labels\", is_sep_n=False)\n",
    "val_dataset = preprocess_df(df=df_val, bert_tokenizer=bert_tokenizer, block_size=512, trunc_doc=50, doc_column_name=\"text\", labels_sum_column_name=\"own_labels\", is_sep_n=False)\n",
    "test_dataset = preprocess_df(df=df_test, bert_tokenizer=bert_tokenizer, block_size=512, trunc_doc=50, doc_column_name=\"text\", labels_sum_column_name=\"own_labels\", is_sep_n=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, prop=0.1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertExtSUMNER(nn.Module):\n",
    "  def __init__(self, bert_layer, bert_tokenizer, dim_emb=768) -> None:\n",
    "    super(BertExtSUMNER, self).__init__()\n",
    "    self.bert_layer = bert_layer\n",
    "    self.bert_tokenizer = bert_tokenizer\n",
    "    self.dim_emb = dim_emb\n",
    "\n",
    "    # predict summary\n",
    "    self.w_sum = nn.Linear(dim_emb, 1)\n",
    "    \n",
    "    # NER\n",
    "    self.w_ner = nn.Linear(dim_emb, 1)\n",
    "\n",
    "  def forward(self, list_input_ids, list_attention_mask):\n",
    "    id_sep = bert_tokenizer.sep_token_id\n",
    "    id_pad = bert_tokenizer.pad_token\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for i in range(len(list_input_ids)):\n",
    "      input_ids.append(list_input_ids[i].to(self.bert_layer.device))\n",
    "      attention_mask.append(list_attention_mask[i].to(self.bert_layer.device))\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_mask = torch.cat(attention_mask, dim=0)\n",
    "\n",
    "    x = self.bert_layer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    mask_sep = (input_ids == id_sep).view(-1)\n",
    "    mask_not_sep = (torch.ne(input_ids, bert_tokenizer.pad_token_id) & torch.ne(input_ids, bert_tokenizer.sep_token_id)).view(-1)\n",
    "    x = x.last_hidden_state\n",
    "    x = x.view(-1, x.size(-1))\n",
    "    emb_sent = x[mask_sep, :]\n",
    "    emb_entities = x[mask_not_sep, :]\n",
    "\n",
    "    o_sum = self.w_sum(emb_sent)\n",
    "    o_sum = torch.sigmoid(o_sum).squeeze(-1)\n",
    "\n",
    "    o_ner = self.w_ner(emb_entities)\n",
    "    o_ner = torch.sigmoid(o_ner).squeeze(-1)\n",
    "\n",
    "    return o_sum, o_ner\n",
    "\n",
    "  def save(self, fname):\n",
    "    torch.save(self.state_dict(), fname)\n",
    "\n",
    "  def load(self, fname):\n",
    "    self.load_state_dict(torch.load(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertExtSUMNER(bert_layer=bert_layer, bert_tokenizer=bert_tokenizer)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(checkpoints_folder):\n",
    "  os.makedirs(checkpoints_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[\"doc_splitted\"] = split_all_docs(df_val[\"text\"])\n",
    "val_set = df_val\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 3batch [00:01,  1.96batch/s, loss=0.134, loss_sum=0.134]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : val loss = 0.204, val loss summary = 0.204, r1 = 0.418, r2 = 0.175, rL = 0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 3batch [00:01,  2.68batch/s, loss=2.01, loss_sum=2.01]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 : val loss = 6.257, val loss summary = 6.257, r1 = 0.409, r2 = 0.166, rL = 0.244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 3batch [00:01,  2.67batch/s, loss=0.963, loss_sum=0.963]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 : val loss = 0.189, val loss summary = 0.189, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 3batch [00:01,  2.36batch/s, loss=0.395, loss_sum=0.395]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 : val loss = 0.163, val loss summary = 0.163, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: : 3batch [00:00,  3.09batch/s, loss=0.334, loss_sum=0.334]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 : val loss = 0.117, val loss summary = 0.117, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: : 3batch [00:01,  2.33batch/s, loss=0.299, loss_sum=0.299]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 : val loss = 0.130, val loss summary = 0.130, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: : 3batch [00:01,  2.38batch/s, loss=1.11, loss_sum=1.11]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 : val loss = 0.104, val loss summary = 0.104, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: : 3batch [00:00,  3.07batch/s, loss=0.0178, loss_sum=0.0178]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 : val loss = 0.118, val loss summary = 0.118, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: : 3batch [00:01,  2.67batch/s, loss=0.00421, loss_sum=0.00421]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 : val loss = 0.140, val loss summary = 0.140, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: : 3batch [00:01,  2.92batch/s, loss=0.00165, loss_sum=0.00165]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 : val loss = 0.153, val loss summary = 0.153, r1 = 0.389, r2 = 0.150, rL = 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: : 3batch [00:01,  2.81batch/s, loss=0.232, loss_sum=0.232]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 : val loss = 0.151, val loss summary = 0.151, r1 = 0.389, r2 = 0.150, rL = 0.227\n",
      "Training duration = 63.14715123176575\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "arr_train_loss = []\n",
    "arr_train_loss_sum = []\n",
    "#arr_train_loss_ner = []\n",
    "#arr_train_acc_sum = []\n",
    "#arr_train_acc_ner = []\n",
    "arr_val_loss = []\n",
    "#arr_val_acc_sum = []\n",
    "#arr_val_acc_ner = []\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "  # Train\n",
    "  model.train()\n",
    "  nb_batch_train = 0\n",
    "  nb_loss_train = 0\n",
    "  total_train_loss = 0\n",
    "  total_train_loss_sum = 0\n",
    "  #total_train_loss_ner = 0\n",
    "  #total_train_acc_sum = 0\n",
    "  #total_train_acc_ner = 0\n",
    "  \n",
    "  id_sep = bert_tokenizer.sep_token_id\n",
    "  id_pad = bert_tokenizer.pad_token\n",
    "\n",
    "  for i in range(len(train_loader)):\n",
    "    train_loader[i]\n",
    "\n",
    "  with tqdm(train_loader, unit=\"batch\", total=len(train_loader)) as tepoch:\n",
    "    for batch in tepoch:\n",
    "      tepoch.set_description(f\"Epoch {epoch}\")\n",
    "      #if dev != \"cpu\":\n",
    "      #  torch.cuda.empty_cache()\n",
    "      list_input_ids = batch[\"input_ids\"]\n",
    "      list_attention_mask = batch[\"attention_mask\"]\n",
    "      list_targets_sum = batch[\"labels\"]\n",
    "      #list_targets_ner = batch[\"labels_ner\"]\n",
    "      \n",
    "      list_y_sum_pred = []\n",
    "      #list_y_ner_pred = []\n",
    "      for i in range(len(list_input_ids)):\n",
    "        y_sum_pred, y_ner_pred = model(list_input_ids[i:i+1], list_attention_mask[i:i+1])\n",
    "\n",
    "        loss_sum = criterion(y_sum_pred, torch.tensor(list_targets_sum[i], dtype=torch.float).to(device))\n",
    "        #loss_ner = criterion(y_ner_pred, torch.tensor(list_targets_ner[i], dtype=torch.float).to(device))\n",
    "        \n",
    "        #loss = loss_sum_coef * loss_sum + loss_ner_coef * loss_ner\n",
    "        loss = loss_sum\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        nb_loss_train += 1\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_loss_sum += loss_sum.item()\n",
    "        #total_train_loss_ner += loss_ner.item()\n",
    "\n",
    "        list_y_sum_pred.append(y_sum_pred.detach())\n",
    "        #list_y_ner_pred.append(y_ner_pred.detach())\n",
    "\n",
    "      nb_batch_train += 1\n",
    "\n",
    "      y_sum_pred = torch.cat(list_y_sum_pred)\n",
    "      #y_ner_pred = torch.cat(list_y_ner_pred)\n",
    "      targets_sum = torch.cat([torch.tensor(e, dtype=torch.float).to(device) for e in list_targets_sum])\n",
    "      #targets_ner = torch.cat([torch.tensor(e, dtype=torch.float).to(device) for e in list_targets_ner])\n",
    "\n",
    "      probs = y_sum_pred.tolist() # compute_probs(y_pred)\n",
    "      probs_tensor = y_sum_pred #torch.tensor(probs, dtype=torch.float).to(device)\n",
    "\n",
    "      #total_train_acc_ner += torch.sum(((y_ner_pred > 0.5).float() == targets_ner).float()).item() / targets_ner.shape[0]\n",
    "\n",
    "      tepoch.set_postfix(loss=total_train_loss/nb_loss_train, loss_sum=total_train_loss_sum/nb_loss_train)\n",
    "\n",
    "  # Save model\n",
    "  model.save(checkpoints_folder + \"/\" + model_name + \"-\" + str(epoch) + \".pt\")\n",
    "\n",
    "  # Eval\n",
    "  model.eval()\n",
    "  nb_batch_val = 0\n",
    "  nb_loss_val = 0\n",
    "  total_val_loss = 0\n",
    "  total_val_loss_sum = 0\n",
    "  #total_val_loss_ner = 0\n",
    "  #total_val_acc_sum = 0\n",
    "  #total_val_acc_ner = 0\n",
    "  total_r1 = 0\n",
    "  total_r2 = 0\n",
    "  total_rl = 0\n",
    "\n",
    "  del loss\n",
    "  del loss_sum\n",
    "  del y_sum_pred\n",
    "  del y_ner_pred\n",
    "\n",
    "  if dev != \"cpu\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  for i, batch in enumerate(val_loader):\n",
    "    #if dev != \"cpu\":\n",
    "    #  torch.cuda.empty_cache()\n",
    "    list_input_ids = batch[\"input_ids\"]\n",
    "    list_attention_mask = batch[\"attention_mask\"]\n",
    "    list_targets_sum = batch[\"labels\"]\n",
    "\n",
    "    list_y_sum_pred = []\n",
    "    #list_y_ner_pred = []\n",
    "    for j in range(len(list_input_ids)):\n",
    "      y_sum_pred, y_ner_pred = model(list_input_ids[j:j+1], list_attention_mask[j:j+1])\n",
    "\n",
    "      loss_sum = criterion(y_sum_pred, torch.tensor(list_targets_sum[j], dtype=torch.float).to(device))\n",
    "      #loss_ner = criterion(y_ner_pred, torch.tensor(list_targets_ner[j], dtype=torch.float).to(device))\n",
    "      \n",
    "      #loss = loss_sum_coef * loss_sum + loss_ner_coef * loss_ner\n",
    "      loss = loss_sum\n",
    "\n",
    "      nb_loss_val += 1      \n",
    "      total_val_loss += loss.item()\n",
    "      total_val_loss_sum += loss_sum.item()\n",
    "      #total_val_loss_ner += loss_ner.item()\n",
    "\n",
    "      list_y_sum_pred.append(y_sum_pred.detach())\n",
    "      #list_y_ner_pred.append(y_ner_pred.detach())\n",
    "\n",
    "    nb_batch_val += 1\n",
    "\n",
    "    y_sum_pred = torch.cat(list_y_sum_pred)\n",
    "    #y_ner_pred = torch.cat(list_y_ner_pred)\n",
    "    targets_sum = torch.cat([torch.tensor(e, dtype=torch.float).to(device) for e in list_targets_sum])\n",
    "    #targets_ner = torch.cat([torch.tensor(e, dtype=torch.float).to(device) for e in list_targets_ner])\n",
    "\n",
    "    doc = val_set[\"doc_splitted\"].iloc[i]\n",
    "    summaries = val_set[\"summaries\"].iloc[i]\n",
    "\n",
    "    indices = torch.argsort(y_sum_pred, descending=True)\n",
    "\n",
    "    y_pred_thresh = []\n",
    "\n",
    "    txt = \"\"\n",
    "\n",
    "    doc_lens = [len(doc)]\n",
    "    for j in range(doc_lens[0]):\n",
    "      txt = txt + \". \" + doc[indices[j]]\n",
    "      y_pred_thresh.append(indices[j])\n",
    "      if len(txt) >= len(summaries):\n",
    "        break\n",
    "\n",
    "    y_pred_thresh.sort()\n",
    "\n",
    "    txt = \"\"\n",
    "\n",
    "    for j in y_pred_thresh:\n",
    "      txt = txt + \". \" + doc[j]\n",
    "\n",
    "    n = min(len(txt), len(summaries))\n",
    "\n",
    "    while n < len(txt) and txt[n].isalnum():\n",
    "      n += 1\n",
    "\n",
    "    txt = txt[:n]\n",
    "\n",
    "    scores = scorer.score(summaries, txt)\n",
    "    total_r1 += scores[\"rouge1\"].recall\n",
    "    total_r2 += scores[\"rouge2\"].recall\n",
    "    total_rl += scores[\"rougeL\"].recall\n",
    "\n",
    "    probs = y_sum_pred.tolist() # compute_probs(y_pred)\n",
    "    probs_tensor = y_sum_pred #torch.tensor(probs, dtype=torch.float).to(device)\n",
    "\n",
    "    #total_val_acc_sum += accuracy_nb_sent_per_doc_fn(probs=probs, targets=targets_sum.cpu().detach().numpy(), doc_lens=doc_lens, average_number_of_sentences_per_document=average_number_of_sentences_per_document)\n",
    "    #total_val_acc_ner += torch.sum(((y_ner_pred > 0.5).float() == targets_ner).float()).item() / targets_ner.shape[0]\n",
    "\n",
    "  print(\"Epoch {} : val loss = {:.3f}, val loss summary = {:.3f}, r1 = {:.3f}, r2 = {:.3f}, rL = {:.3f}\".format(epoch, total_val_loss / nb_loss_val, total_val_loss_sum / nb_loss_val, total_r1 / nb_batch_val, total_r2 / nb_batch_val, total_rl / nb_batch_val))\n",
    "\n",
    "  if len(arr_val_loss) >= early_stopping+1:\n",
    "    if min(arr_val_loss[-early_stopping:]) >= arr_val_loss[-(early_stopping+1)]:\n",
    "      break\n",
    "\n",
    "  del loss\n",
    "  del loss_sum\n",
    "  del y_sum_pred\n",
    "  del y_ner_pred\n",
    "\n",
    "  if dev != \"cpu\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  arr_train_loss.append(total_train_loss / nb_batch_train)\n",
    "  \n",
    "  #arr_train_acc_sum.append(total_train_acc_sum / nb_batch_train)\n",
    "  #arr_train_acc_ner.append(total_train_acc_ner / nb_batch_train)\n",
    "\n",
    "  arr_val_loss.append(total_val_loss / nb_batch_val)\n",
    "  #arr_val_acc_sum.append(total_val_acc_sum / nb_batch_val)\n",
    "  #arr_val_acc_ner.append(total_val_acc_ner / nb_batch_val)\n",
    "\n",
    "t2 = time()\n",
    "print(\"Training duration =\", t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = {}\n",
    "training_metrics[\"duration\"]   = t2 - t1\n",
    "training_metrics[\"train_loss\"] = arr_train_loss\n",
    "#training_metrics[\"train_acc_sum\"]  = arr_train_acc_sum\n",
    "#training_metrics[\"train_acc_ner\"]  = arr_train_acc_ner\n",
    "training_metrics[\"val_loss\"]   = arr_val_loss\n",
    "#training_metrics[\"val_acc_sum\"]    = arr_val_acc_sum\n",
    "#training_metrics[\"val_acc_ner\"]    = arr_val_acc_ner\n",
    "\n",
    "# Save to file in JSON format\n",
    "\n",
    "with open(checkpoints_folder + \"/training_metrics.json\", 'w') as fp:\n",
    "  json.dump(training_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw Losses\n",
    "if is_graphic:\n",
    "  plt.plot(list(range(1, len(arr_train_loss) + 1)), arr_train_loss, label=\"train\")\n",
    "  plt.plot(list(range(1, len(arr_val_loss) + 1)), arr_val_loss, label=\"val\")\n",
    "\n",
    "  plt.title(\"Loss\")\n",
    "  plt.xlabel(\"epoch\")\n",
    "  plt.ylabel(\"loss\")\n",
    "  plt.legend(loc=\"upper left\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw Accuracies\n",
    "if False and is_graphic:\n",
    "  plt.plot(list(range(1, len(arr_train_acc_sum) + 1)), arr_train_acc_sum, label=\"train\")\n",
    "  plt.plot(list(range(1, len(arr_val_acc_sum) + 1)), arr_val_acc_sum, label=\"val\")\n",
    "\n",
    "  plt.title(\"Accuracy Summary\")\n",
    "  plt.xlabel(\"epoch\")\n",
    "  plt.ylabel(\"accuracy\")\n",
    "  plt.legend(loc=\"upper left\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw Accuracies\n",
    "if False and is_graphic:\n",
    "  plt.plot(list(range(1, len(arr_train_acc_ner) + 1)), arr_train_acc_ner, label=\"train\")\n",
    "  plt.plot(list(range(1, len(arr_val_acc_ner) + 1)), arr_val_acc_ner, label=\"val\")\n",
    "\n",
    "  plt.title(\"Accuracy NER\")\n",
    "  plt.xlabel(\"epoch\")\n",
    "  plt.ylabel(\"accuracy\")\n",
    "  plt.legend(loc=\"upper left\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertExtSUMNER(bert_layer=bert_layer, bert_tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch = np.argmin(np.array(arr_val_loss)) + 1\n",
    "model.load(checkpoints_folder + \"/\" + model_name + \"-\" + str(best_epoch) + \".pt\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"doc_splitted\"] = split_all_docs(df_test[\"text\"], False)\n",
    "test_set = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model: 100%|| 100/100 [00:03<00:00, 27.10batch/s, loss=0.126, loss_sum=0.126, rouge1=0.363, rouge2=0.139, rougeL=0.227] \n"
     ]
    }
   ],
   "source": [
    "arr_rouge1 = []\n",
    "arr_rouge2 = []\n",
    "arr_rougeL = []\n",
    "arr_loss = []\n",
    "arr_loss_sum = []\n",
    "#arr_loss_ner = []\n",
    "#accuracy_sum = []\n",
    "#accuracy_ner = []\n",
    "\n",
    "idx = 0\n",
    "\n",
    "counter = 0\n",
    "with tqdm(test_loader, unit=\"batch\", total=len(test_loader)) as tepoch:\n",
    "  for batch in tepoch:\n",
    "    tepoch.set_description(\"Eval model\")\n",
    "    list_input_ids = batch[\"input_ids\"]\n",
    "    list_attention_mask = batch[\"attention_mask\"]\n",
    "    list_targets_sum = batch[\"labels\"]\n",
    "\n",
    "    list_y_sum_pred = []\n",
    "    #list_y_ner_pred = []\n",
    "    for j in range(len(list_input_ids)):\n",
    "      y_sum_pred, y_ner_pred = model(list_input_ids[j:j+1], list_attention_mask[j:j+1])\n",
    "\n",
    "      loss_sum = criterion(y_sum_pred, torch.tensor(list_targets_sum[j], dtype=torch.float).to(device))\n",
    "      #loss_ner = criterion(y_ner_pred, torch.tensor(list_targets_ner[j], dtype=torch.float).to(device))\n",
    "    \n",
    "      #loss = loss_sum_coef * loss_sum + loss_ner_coef * loss_ner\n",
    "      loss = loss_sum\n",
    "\n",
    "      arr_loss.append(loss.item())\n",
    "      arr_loss_sum.append(loss_sum.item())\n",
    "      #arr_loss_ner.append(loss_ner.item())\n",
    "\n",
    "      list_y_sum_pred.append(y_sum_pred.detach())\n",
    "      #list_y_ner_pred.append(y_ner_pred.detach())\n",
    "\n",
    "    y_sum_pred = torch.cat(list_y_sum_pred)\n",
    "    #y_ner_pred = torch.cat(list_y_ner_pred)\n",
    "    #targets_sum = torch.cat([torch.tensor(e, dtype=torch.float).to(device) for e in list_targets_sum])\n",
    "\n",
    "    doc = test_set[\"doc_splitted\"].iloc[idx]\n",
    "    summaries = test_set[\"summaries\"].iloc[idx]\n",
    "\n",
    "    probs = np.array(y_sum_pred.tolist()) # compute_probs(y_pred)\n",
    "    probs_tensor = y_sum_pred #torch.tensor(probs, dtype=torch.float).to(device)\n",
    "\n",
    "    #probs = threshold_probs_by_nb(probs=probs, doc_lens=[probs.shape[0]], average_number_of_sentences_per_document=average_number_of_sentences_per_document)\n",
    "    #probs = threshold_probs_by_prop(probs=probs, doc_lens=[probs.shape[0]], average_proportion_of_sentences_per_document=average_proportion_of_sentences_per_document)\n",
    "    indices = torch.argsort(y_sum_pred, descending=True)\n",
    "\n",
    "    y_pred_thresh = []\n",
    "\n",
    "    txt = \"\"\n",
    "\n",
    "    for i in range(min(len(doc), y_sum_pred.shape[0])):\n",
    "      txt = txt + \". \" + doc[indices[i]]\n",
    "      y_pred_thresh.append(indices[i])\n",
    "      if len(txt) >= len(summaries):\n",
    "        break\n",
    "\n",
    "    y_pred_thresh.sort()\n",
    "\n",
    "    txt = \"\"\n",
    "\n",
    "    for i in y_pred_thresh:#range(min(len(doc), y_pred.shape[0])):\n",
    "      txt = txt + \". \" + doc[i]\n",
    "\n",
    "    n = min(len(txt), len(summaries))\n",
    "\n",
    "    while n < len(txt) and txt[n].isalnum():\n",
    "      n += 1\n",
    "\n",
    "    txt = txt[:n]\n",
    "\n",
    "    # assert len(txt) - len(summaries) <= 20\n",
    "\n",
    "    scores = scorer.score(summaries, txt)\n",
    "    arr_rouge1.append(scores[\"rouge1\"].recall)\n",
    "    arr_rouge2.append(scores[\"rouge2\"].recall)\n",
    "    arr_rougeL.append(scores[\"rougeL\"].recall)\n",
    "\n",
    "    #accuracy_sum.append(accuracy_nb_sent_per_doc_fn(probs=probs, targets=targets_sum.cpu().detach().numpy(), doc_lens=[len(probs)], average_number_of_sentences_per_document=average_number_of_sentences_per_document))\n",
    "    #accuracy.append(accuracy_prop_sent_per_doc_fn(probs=probs, targets=targets.cpu().detach().numpy(), doc_lens=[len(probs)], average_proportion_of_sentences_per_document=average_proportion_of_sentences_per_document))\n",
    "    #accuracy_ner.append(torch.sum(((y_ner_pred > 0.5).float() == targets_ner).float()).item() / targets_ner.shape[0])\n",
    "\n",
    "    tepoch.set_postfix(loss=average(arr_loss), loss_sum=average(arr_loss_sum), rouge1 = average(arr_rouge1), rouge2 = average(arr_rouge2), rougeL = average(arr_rougeL))\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {}\n",
    "#test_metrics[\"accuracy_sum\"] = average(accuracy_sum)\n",
    "#test_metrics[\"accuracy_ner\"] = average(accuracy_ner)\n",
    "test_metrics[\"rouge1\"]   = average(arr_rouge1)\n",
    "test_metrics[\"rouge2\"]   = average(arr_rouge2)\n",
    "test_metrics[\"rougeL\"]   = average(arr_rougeL)\n",
    "\n",
    "# Save to file in JSON format\n",
    "\n",
    "with open(checkpoints_folder + \"/test_metrics.json\", 'w') as fp:\n",
    "  json.dump(test_metrics, fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lead-3: 100%|| 100/100 [00:00<00:00, 141.63batch/s, rouge1=0.409, rouge2=0.172, rougeL=0.254]\n"
     ]
    }
   ],
   "source": [
    "arr_rouge1 = []\n",
    "arr_rouge2 = []\n",
    "arr_rougeL = []\n",
    "accuracy = []\n",
    "\n",
    "idx = 0\n",
    "with tqdm(test_loader, unit=\"batch\", total=len(test_loader)) as tepoch:\n",
    "    for batch in tepoch:\n",
    "        tepoch.set_description(\"Lead-3\")\n",
    "        doc = test_set[\"doc_splitted\"].iloc[idx]\n",
    "\n",
    "        txt = \"\"\n",
    "\n",
    "        for i in range(min(len(doc), 3)):\n",
    "            txt = txt + doc[i]\n",
    "\n",
    "        summaries = test_set[\"summaries\"].iloc[idx]\n",
    "\n",
    "        n = min(len(txt), len(summaries))\n",
    "\n",
    "        while n < len(txt) and txt[n].isalnum():\n",
    "            n += 1\n",
    "\n",
    "        txt = txt[:n]\n",
    "\n",
    "        # assert len(txt) - len(summaries) <= 20\n",
    "\n",
    "        scores = scorer.score(summaries, txt)\n",
    "        arr_rouge1.append(scores[\"rouge1\"].recall)\n",
    "        arr_rouge2.append(scores[\"rouge2\"].recall)\n",
    "        arr_rougeL.append(scores[\"rougeL\"].recall)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "        tepoch.set_postfix(rouge1 = average(arr_rouge1), rouge2 = average(arr_rouge2), rougeL = average(arr_rougeL))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First n char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "First-n-char': 100%|| 100/100 [00:00<00:00, 141.54batch/s, rouge1=0.409, rouge2=0.172, rougeL=0.254]\n"
     ]
    }
   ],
   "source": [
    "arr_rouge1 = []\n",
    "arr_rouge2 = []\n",
    "arr_rougeL = []\n",
    "accuracy = []\n",
    "\n",
    "idx = 0\n",
    "with tqdm(test_loader, unit=\"batch\", total=len(test_loader)) as tepoch:\n",
    "    for batch in tepoch:\n",
    "        tepoch.set_description(\"First-n-char'\")\n",
    "        doc = test_set[\"doc_splitted\"].iloc[idx]\n",
    "\n",
    "        txt = \"\"\n",
    "\n",
    "        for i in range(len(doc)):\n",
    "            txt = txt + doc[i]\n",
    "\n",
    "        summaries = test_set[\"summaries\"].iloc[idx]\n",
    "\n",
    "        n = min(len(txt), len(summaries))\n",
    "\n",
    "        while n < len(txt) and txt[n].isalnum():\n",
    "            n += 1\n",
    "\n",
    "        txt = txt[:n]\n",
    "\n",
    "        scores = scorer.score(summaries, txt)\n",
    "        arr_rouge1.append(scores[\"rouge1\"].recall)\n",
    "        arr_rouge2.append(scores[\"rouge2\"].recall)\n",
    "        arr_rougeL.append(scores[\"rougeL\"].recall)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "        tepoch.set_postfix(rouge1 = average(arr_rouge1), rouge2 = average(arr_rouge2), rougeL = average(arr_rougeL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raoufdine/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Does not execute this cell if you want to execute the following cells.\n",
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(train_dataset[0][\"input_ids\"] == 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0][\"labels_sum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2474, 5715, 9765, 8740, 13926, 1011, 9765, 4241, 17155, 16778, 2078, 1012, 102, 2365, 8945, 12514, 9765, 1037, 1016, 1010, 1019, 2463, 1037, 1048, 1005, 9765, 2139, 3002, 1011, 5578, 1011, 1041, 25394, 3366, 3802, 1037, 1022, 2463, 1037, 1048, 1005, 15068, 4355, 2139, 3347, 21031, 3126, 1012, 102, 3393, 18856, 9581, 2102, 21864, 14418, 21162, 5562, 2474, 5715, 9765, 24209, 11475, 8873, 2063, 1010, 4372, 2230, 1010, 2139, 1077, 18856, 9581, 2102, 4153, 7413, 23151, 2278, 1090, 1010, 7367, 7811, 2474, 5939, 18155, 8649, 2666, 4078, 18856, 9581, 3215, 2139, 2474, 2605, 21864, 4012, 13876, 2063, 2632, 5668, 17504, 2102, 2882, 2015, 4127, 2139, 18856, 9581, 3215, 4372, 6005, 15049, 1012, 102, 4372, 12609, 1010, 2474, 5715, 24501, 21748, 2102, 4241, 2828, 1077, 18856, 9581, 2102, 4153, 7413, 1090, 18033, 2474, 5579, 27859, 16558, 2666, 11968, 2777, 8780, 1011, 2605, 1010, 21864, 11265, 4012, 13876, 2063, 4078, 2953, 2863, 2483, 1010, 4372, 6765, 10439, 3217, 5403, 1010, 10861, 25022, 2078, 4160, 2882, 2015, 4127, 2139, 18856, 9581, 3215, 4372, 6005, 15049, 1012, 102, 8292, 2828, 2139, 18856, 9581, 2102, 7367, 19817, 4215, 14663, 11968, 4078, 7715, 2079, 18796, 2015, 3802, 16655, 20228, 2226, 25500, 11368, 7373, 5816, 3672, 11113, 29067, 10111, 1006, 4372, 14975, 13642, 2278, 4649, 2566, 20689, 23757, 2015, 2310, 16885, 2139, 1048, 1005, 2012, 5802, 28437, 1007, 1010, 16360, 8445, 2666, 2000, 4904, 8740, 2146, 2139, 1048, 1005, 4776, 2063, 13642, 2278, 4895, 26523, 4555, 1040, 1005, 13323, 16429, 2890, 1037, 10768, 19716, 3771, 1012, 102, 4649, 11498, 11368, 6072, 18856, 9581, 28437, 2015, 21864, 2006, 2102, 2566, 15630, 1040, 1521, 27859, 16558, 4313, 2474, 5939, 18155, 8649, 2666, 2139, 2230, 4012, 6442, 4765, 2416, 10857, 10364, 4649, 7715, 3802, 17504, 2102, 10364, 4649, 13511, 2015, 1010, 2123, 2102, 4649, 10380, 9236, 11370, 1037, 2474, 3671, 2063, 3411, 1011, 2456, 1012, 102, 4649, 17419, 4054, 2229, 10857, 14418, 21162, 29196, 2102, 2474, 5715, 2365, 2102, 2556, 10285, 18033, 1048, 1005, 4372, 3540, 16200, 25022, 1011, 19804, 2229, 1012, 102, 13642, 2278, 3393, 2689, 3672, 18856, 9581, 28437, 1010, 8292, 2015, 10857, 2006, 2102, 23408, 4747, 5657, 1012, 102, 16655, 3802, 12672, 19148, 2063, 4372, 2297, 11968, 2474, 3257, 2236, 2063, 2139, 1048, 1005, 4372, 2121, 11239, 3802, 4241, 18856, 9581, 2102, 3143, 2063, 11968, 4078, 25041, 3164, 2229, 3653, 6767, 4183, 4372, 1041, 16020, 2102, 10861, 2474, 4860, 9587, 20684, 2638, 16475, 14995, 2102, 13675, 28100, 2890, 3802, 2474, 20228, 2226, 25500, 11368, 7373, 9587, 20684, 2638, 21790, 18116, 1010, 13642, 2278, 2000, 10421, 14876, 2483, 2139, 24898, 2015, 8358, 3164, 2229, 1012, 102, 8292, 2015, 2689, 8163, 21877, 27346, 2102, 3802, 2890, 9530, 9153, 4570, 7505, 2474, 2276, 23879, 12898, 5856, 4226, 2139, 2777, 8780, 1011, 2605, 2474, 4606, 4013, 5403, 1010, 1077, 2175, 10087, 3077, 1090, 1010, 7505, 2474, 5715, 2139, 2175, 10087, 3077, 1011, 3393, 1996, 4014, 1010, 28616, 2063, 4372, 2326, 4372, 3851, 3802, 21864, 7367, 19817, 7140, 3726, 1037, 1023, 2463, 1037, 5285, 1040, 1005, 1051, 5562, 4887, 1010, 1010, 15068, 2474, 4860, 9587, 20684, 2638, 5754, 16284, 2571, 9765, 2139, 2184, 1010, 1021, 6362, 3802, 2474, 18535, 3126, 2139, 13511, 2015, 2139, 6205, 2683, 1010, 1021, 3461, 10364, 2474, 2558, 2063, 3261, 1011, 2230, 1012, 102, 7505, 2474, 2276, 23879, 12898, 5856, 4226, 2010, 29469, 4226, 2474, 4606, 4013, 5403, 1010, 1077, 24188, 20431, 1516, 5003, 6279, 8743, 2271, 1090, 1010, 7505, 2474, 5715, 2139, 24188, 20431, 1011, 4372, 1011, 17155, 16778, 2078, 1010, 28616, 2063, 4372, 2326, 4372, 4437, 3802, 1037, 2539, 2463, 1010, 2474, 4860, 9587, 20684, 2638, 5754, 16284, 2571, 23408, 4747, 5657, 2139, 2184, 1010, 1018, 6362, 10364, 2474, 2558, 2063, 3411, 1011, 2456, 1037, 2184, 1010, 1021, 6362, 10364, 3261, 1011, 2230, 1010, 16405, 2483, 1037, 2340, 1010, 1015, 6362, 10364, 2889, 1011, 12609, 1012, 102, 13075, 7140, 3077, 9765, 16655, 5715, 3541, 2063, 1010, 2482, 15317, 26208, 2102, 2112, 2666, 4078, 16569, 21877, 2226, 15068, 24403, 21877, 2226, 9742, 2015, 1010, 8740, 12411, 2015, 2139, 2474, 26192, 15029, 2063, 2139, 7939, 28032, 2063, 2139, 1048, 1005, 16021, 4402, 1010, 1010, 1010, 1012, 102, 11968, 9932, 6216, 9236, 2474, 5715, 26208, 2102, 2112, 2666, 2139, 1048, 1005, 2250, 2063, 1040, 1005, 8432, 2139, 24188, 20431, 1011, 4372, 1011, 17155, 16778, 2078, 1010, 2123, 2102, 15317, 9765, 16655, 5715, 2139, 2474, 2522, 21017, 2638, 1012, 102, 8292, 4674, 2250, 2063, 1010, 21864, 19723, 22107, 2063, 6255, 16569, 1010, 9765, 4937, 20265, 29346, 2063, 18033, 4649, 9149, 2139, 2753, 2199, 1037, 25175, 3619, 2139, 3263, 2199, 10427, 11390, 1010, 1012, 102, 1048, 1005, 6139, 4078, 14017, 2015, 2139, 2474, 5715, 1010, 2425, 2063, 24209, 1005, 15317, 24501, 21748, 2102, 2139, 2474, 2918, 2139, 2123, 24045, 2015, 2885, 24336, 1040, 1521, 6139, 16012, 21281, 5332, 4226, 4078, 14017, 2015, 2522, 11467, 2455, 3104, 1006, 18856, 2278, 1007, 1010, 9765, 9388, 4226, 2063, 11968, 1048, 1005, 5197, 4078, 26568, 3406, 7442, 2015, 12943, 7277, 29111, 1006, 2531, 1003, 4372, 2760, 1007, 1010, 16655, 10817, 8909, 4765, 7413, 1037, 3526, 2063, 2139, 2901, 1006, 2531, 1003, 1007, 1012, 102, 2474, 16360, 8445, 22753, 6987, 10559, 4372, 2760, 9765, 2474, 24086, 18941, 2063, 1024, 10996, 2015, 1006, 4805, 1003, 1007, 1010, 25170, 2015, 5424, 4244, 1006, 4229, 1010, 1019, 1003, 1007, 1010, 10019, 12943, 7277, 29111, 21770, 10624, 6914, 2229, 1006, 2321, 1010, 1019, 1003, 1007, 1012, 102, 1048, 1005, 16270, 2777, 11968, 9932, 6216, 9236, 1037, 22137, 4895, 2041, 4014, 4372, 5622, 10177, 2566, 11368, 5794, 2102, 2139, 12826, 2099, 1048, 1521, 6622, 18033, 3393, 29023, 2139, 1048, 1521, 6139, 4078, 14017, 2015, 2139, 2474, 5715, 1006, 15068, 2139, 26568, 3406, 7442, 2015, 1037, 4078, 14925, 18223, 2229, 2367, 2229, 1007, 1012, 102, 4606, 17301, 2869, 4958, 2080, 10997, 2365, 2102, 7801, 2015, 27411, 2433, 2063, 2139, 11122, 2229, 15068, 7760, 29347, 23144, 5267, 1024, 2474, 11122, 2063, 2139, 16220, 5498, 1006, 16855, 6137, 2063, 9033, 8586, 2571, 1007, 1010, 2474, 11122, 2063, 1040, 1005, 17997, 1011, 2350, 1006, 11102, 1011, 7647, 1007, 3802, 2474, 2558, 2063, 2552, 16284, 2571, 1006, 3925, 1037, 8740, 23099, 4103, 1005, 17504, 1007, 1012, 102, 3393, 2053, 2213, 2139, 2474, 2334, 4221, 9765, 2012, 22199, 2063, 27411, 4649]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"input_ids\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nLa commune est au nord-est du Cotentin. Son bourg est  2,5 km  l'est de Saint-Pierre-glise et  8 km  l'ouest de Barfleur.\\n\\n\\nLe climat qui caractrise la commune est qualifi, en 2010, de  climat ocanique franc , selon la typologie des climats de la France qui compte alors huit grands types de climats en mtropole. En 2020, la commune ressort du type  climat ocanique  dans la classification tablie par Mto-France, qui ne compte dsormais, en premire approche, que cinq grands types de climats en mtropole. Ce type de climat se traduit par des tempratures douces et une pluviomtrie relativement abondante (en liaison avec les perturbations venant de l'Atlantique), rpartie tout au long de l'anne avec un lger maximum d'octobre  fvrier.\\nLes paramtres climatiques qui ont permis dtablir la typologie de 2010 comportent six variables pour les tempratures et huit pour les prcipitations, dont les valeurs correspondent  la normale 1971-2000. Les sept principales variables caractrisant la commune sont prsentes dans l'encadr ci-aprs.\\n\\nAvec le changement climatique, ces variables ont volu. Une tude ralise en 2014 par la Direction gnrale de l'nergie et du Climat complte par des tudes rgionales prvoit en effet que la temprature moyenne devrait crotre et la pluviomtrie moyenne baisser, avec toutefois de fortes variations rgionales. Ces changements peuvent tre constats sur la station mtorologique de Mto-France la plus proche,  Gonneville , sur la commune de Gonneville-Le Theil, mise en service en 1959 et qui se trouve  9 km  vol d'oiseau,, o la temprature moyenne annuelle est de 10,7 C et la hauteur de prcipitations de 919,7 mm pour la priode 1981-2010.\\nSur la station mtorologique historique la plus proche,  Cherbourg  Maupertus , sur la commune de Cherbourg-en-Cotentin, mise en service en 1935 et  19 km, la temprature moyenne annuelle volue de 10,4 C pour la priode 1971-2000  10,7 C pour 1981-2010, puis  11,1 C pour 1991-2020.\\n\\n\\n\\n\\nVarouville est une commune rurale, car elle fait partie des communes peu ou trs peu denses, au sens de la grille communale de densit de l'Insee,,,.\\nPar ailleurs la commune fait partie de l'aire d'attraction de Cherbourg-en-Cotentin, dont elle est une commune de la couronne. Cette aire, qui regroupe 77 communes, est catgorise dans les aires de 50 000  moins de 200 000 habitants,.\\n\\n\\n\\nL'occupation des sols de la commune, telle qu'elle ressort de la base de donnes europenne doccupation biophysique des sols Corine Land Cover (CLC), est marque par l'importance des territoires agricoles (100 % en 2018), une proportion identique  celle de 1990 (100 %). La rpartition dtaille en 2018 est la suivante : prairies (46 %), terres arables (38,5 %), zones agricoles htrognes (15,5 %).\\nL'IGN met par ailleurs  disposition un outil en ligne permettant de comparer lvolution dans le temps de loccupation des sols de la commune (ou de territoires  des chelles diffrentes). Plusieurs poques sont accessibles sous forme de cartes ou photos ariennes : la carte de Cassini (XVIIIe sicle), la carte d'tat-major (1820-1866) et la priode actuelle (1950  aujourd'hui).\\n\\n\\nLe nom de la localit est attest sous les formes Vasrouvilla (sans date), Warouvilla en 1280, Varrouvilla vers 1280.\\nLe toponyme est bas sur un anthroponyme germanique tel que Warald ou Warulfus,, (forme latinise, comprendre Warulf/Warolf cf. Warulfe Ier d'Uxelles) et sur l'ancien franais ville/vile dans son sens originel de  domaine rural  issu du latin villa rustica.\\nRemarque : le mme nom de personne est attest au moins une seconde fois en Normandie dans Montgaroult (Orne, Mons Warulfi 1063), cette commune se trouvant au sud de l'isoglosse w- / g(u)- (qui est parallle  la ligne Joret en Normandie), d'o le passage de [w] > [g], alors que dans Varouville, il s'agit de l'volution secondaire [w] > [v] qui s'est produite seulement  partir du XIIe sicle.\\nLe gentil est Varouvillais.\\n\\n\\nEntre 1911 et 1950, la commune est traverse par le  Tue-Vaques , le chemin de fer entre Cherbourg et Barfleur, dont on peut encore voir l'ancienne gare  l'architecture du XXe sicle, prs de l'glise.\\n\\n\\n\\nLe conseil municipal est compos de onze membres dont le maire et deux adjoints.\\n\\n\\nL'volution du nombre d'habitants est connue  travers les recensements de la population effectus dans la commune depuis 1793.  partir de 2006, les populations lgales des communes sont publies annuellement par l'Insee. Le recensement repose dsormais sur une collecte d'information annuelle, concernant successivement tous les territoires communaux au cours d'une priode de cinq ans. Pour les communes de moins de 10 000 habitants, une enqute de recensement portant sur toute la population est ralise tous les cinq ans, les populations lgales des annes intermdiaires tant quant  elles estimes par interpolation ou extrapolation. Pour la commune, le premier recensement exhaustif entrant dans le cadre du nouveau dispositif a t ralis en 2005.\\nEn 2020, la commune comptait 233 habitants, en diminution de 12,08 % par rapport  2014 (Manche : 0,97 %, France hors Mayotte : +1,9 %).\\nVarouville a compt jusqu' 519 habitants en 1806.\\n\\n\\n\\n\\n\\nglise Saint-Martin (XIIIe sicle) avec son clocher en btire bti en 1710, dont le mobilier fut fortement endommag pendant la Rvolution. Elle abrite une sculpture charit Saint-Martin avec donateur en pierre calcaire de la fin du XVe classe au titre objet aux monuments historiques. Elle formait  l'origine tympan au-dessus de la porte d'entre de l'glise et fut dplace  l'intrieur de la nef afin d'tre mieux conserve. Le donateur sans tte est reprsent en prire, avec probablement son pouse derrire lui,  gauche du groupe sculpt. .\\nChteau de la Brhoulle.\\nAncienne gare, prs de l'glise, de la ligne de chemin de fer de Cherbourg  Barfleur o s'arrtait le  tue-vaques .\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"flat_contents\"][df_train.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3002, 1011, 5578, 1011, 1041, 25394, 3366]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.encode(\"Saint-Pierre-glise\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"labels_ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "5\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "8\n",
      "0\n",
      "0\n",
      "1\n",
      "7\n",
      "0\n",
      "0\n",
      "6\n",
      "2\n",
      "6\n",
      "4\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "10\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "5\n",
      "4\n",
      "2\n",
      "5\n",
      "8\n",
      "3\n",
      "4\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      "9\n",
      "0\n",
      "0\n",
      "4\n",
      "4\n",
      "3\n",
      "0\n",
      "2\n",
      "5\n",
      "7\n",
      "3\n",
      "4\n",
      "1\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "4\n",
      "14\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "  print(sum(train_dataset[i][\"labels_ner\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID du token [SEP]: 102\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "\n",
    "print(f\"ID du token [SEP]: {sep_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "  print(len(train_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d7f428a150b92572ac46240b6d7ae68586908362b054f21341550673eeb77dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
